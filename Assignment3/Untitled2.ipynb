{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "from DependencyTree import DependencyTree\n",
    "from ParsingSystem import ParsingSystem\n",
    "from Configuration import Configuration\n",
    "import Config\n",
    "import Util\n",
    "\n",
    "layers = 1\n",
    "\n",
    "\"\"\"\n",
    "This script defines a transition-based dependency parser which makes\n",
    "use of a classifier powered by a neural network. The neural network\n",
    "accepts distributed representation inputs: dense, continuous\n",
    "representations of words, their part of speech tags, and the labels\n",
    "which connect words in a partial dependency parse.\n",
    "\n",
    "This is an implementation of the method described in\n",
    "\n",
    "Danqi Chen and Christopher Manning. A Fast and Accurate Dependency Parser Using Neural Networks. In EMNLP 2014.\n",
    "\n",
    "Author: Danqi Chen, Jon Gauthier\n",
    "Modified by: Heeyoung Kwon (2017)\n",
    "Modified by: Jun S. Kang (2018 Mar)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class DependencyParserModel(object):\n",
    "\n",
    "    def __init__(self, graph, embedding_array, Config):\n",
    "\n",
    "        self.build_graph(graph, embedding_array, Config)\n",
    "\n",
    "\n",
    "    def build_graph(self, graph, embedding_array, Config):\n",
    "        \"\"\"\n",
    "\n",
    "        :param graph:\n",
    "        :param embedding_array:\n",
    "        :param Config:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        with graph.as_default():\n",
    "            self.embeddings = tf.Variable(embedding_array, dtype=tf.float32)\n",
    "#            self.embeddings = tf.Variable(embedding_array, dtype=tf.float32, trainable=False)\n",
    "\n",
    "            \"\"\"\n",
    "            ===================================================================\n",
    "\n",
    "            Define the computational graph with necessary variables.\n",
    "            \n",
    "            1) You may need placeholders of:\n",
    "                - Many parameters are defined at Config: batch_size, n_Tokens, etc\n",
    "                - # of transitions can be get by calling parsing_system.numTransitions()\n",
    "                \n",
    "            self.train_inputs = \n",
    "            self.train_labels = \n",
    "            self.test_inputs =\n",
    "            ...\n",
    "            \n",
    "                \n",
    "            2) Call forward_pass and get predictions\n",
    "            \n",
    "            ...\n",
    "            self.prediction = self.forward_pass(embed, weights_input, biases_input, weights_output)\n",
    "\n",
    "\n",
    "            3) Implement the loss function described in the paper\n",
    "             - lambda is defined at Config.lam\n",
    "            \n",
    "            ...\n",
    "            self.loss =\n",
    "            \n",
    "            ===================================================================\n",
    "            \"\"\"\n",
    "\n",
    " \n",
    "            self.train_inputs =  tf.placeholder(dtype=tf.int32, shape=(Config.batch_size,Config.n_Tokens), name='train_inputs')\n",
    "            self.train_labels = tf.placeholder(dtype=tf.int32, shape=(Config.batch_size,parsing_system.numTransitions()), name='train_labels')\n",
    "            self.test_inputs =  tf.placeholder(dtype=tf.int32, shape=(Config.n_Tokens), name='test_inputs')\n",
    "\n",
    "\n",
    "            embed = tf.nn.embedding_lookup(self.embeddings, self.train_inputs)\n",
    "            embed = tf.reshape(embed, [Config.batch_size, -1])\n",
    "            \n",
    "            # For test data, we only need to get its prediction\n",
    "            test_embed = tf.nn.embedding_lookup(self.embeddings, self.test_inputs)\n",
    "            test_embed = tf.reshape(test_embed, [1, -1])\n",
    "\n",
    "\n",
    "            '''\n",
    "            # weight init by random_uniform_initializer\n",
    "\n",
    "            weights_input = self.random_uniform_initializer((Config.hidden_size, Config.n_Tokens * Config.embedding_size), \"weights_input\",0.01, trainable=True)\n",
    "            biases_input = self.random_uniform_initializer((Config.hidden_size,1), \"biases_input\", 0.01, trainable=True)\n",
    "            weights_output = self.random_uniform_initializer((parsing_system.numTransitions(),Config.hidden_size), \"weights_output\",0.01, trainable=True)\n",
    "            '''\n",
    "            \n",
    "            '''\n",
    "            # weight init by random normal\n",
    "            weights_input = tf.Variable(tf.random.normal([Config.hidden_size, Config.n_Tokens * Config.embedding_size],    mean=0.0,stddev=0.005),\n",
    "                                        name= 'weights_input')\n",
    "             \n",
    "            biases_input = tf.Variable( tf.random.normal([Config.hidden_size,1], mean=0.0,stddev=0.005),\n",
    "                                        name= 'biases_input')\n",
    "             \n",
    "            weights_output = tf.Variable(tf.random.normal([parsing_system.numTransitions(),Config.hidden_size], mean=0.0,stddev=0.005),\n",
    "                                        name= 'weights_output')\n",
    "            ''' \n",
    "             \n",
    "            # weight init by trucated normal\n",
    "            weights_input = tf.Variable(tf.truncated_normal([Config.hidden_size, Config.n_Tokens * Config.embedding_size], mean=0.0,stddev=0.005),\n",
    "                                        name= 'weights_input')\n",
    "            \n",
    "            biases_input = tf.Variable( tf.truncated_normal([Config.hidden_size,1], mean=0.0,stddev=0.05),\n",
    "                                        name= 'biases_input')\n",
    "            \n",
    "            l2_loss = None\n",
    "            \n",
    "            # single layer forward pass function            \n",
    "            if layers ==1 :\n",
    "                weights_output = tf.Variable(tf.truncated_normal([parsing_system.numTransitions(),Config.hidden_size], mean=0.0,stddev=0.005),\n",
    "                                            name= 'weights_output')            \n",
    "                self.prediction = self.forward_pass(embed, weights_input, biases_input, weights_output)\n",
    "                self.test_pred = self.forward_pass(test_embed, weights_input, biases_input, weights_output)\n",
    "                l2_loss = tf.reduce_mean(tf.math.add_n([\n",
    "                        tf.nn.l2_loss(embed),\n",
    "                        tf.nn.l2_loss(weights_input),\n",
    "                        tf.nn.l2_loss(biases_input),\n",
    "                        tf.nn.l2_loss(weights_output),                    \n",
    "                        ],name='adding_l2_losses'))\n",
    "            \n",
    "            # two layer forward pass architecture\n",
    "            if layers ==2 :\n",
    "                weights_input2 = tf.Variable(tf.truncated_normal([Config.hidden_size2, Config.hidden_size],    mean=0.0,stddev=0.005),\n",
    "                                            name= 'weights_input2')\n",
    "                biases_input2 = tf.Variable( tf.truncated_normal([Config.hidden_size2,1], mean=0.0,stddev=0.005),\n",
    "                                            name= 'biases_input2')\n",
    "                weights_output2 = tf.Variable(tf.truncated_normal([parsing_system.numTransitions(),Config.hidden_size2], mean=0.0,stddev=0.005),\n",
    "                                            name= 'weights_output2')\n",
    "                self.prediction = self.forward_pass2Layers(embed, weights_input, biases_input,weights_input2, biases_input2, weights_output2)\n",
    "                self.test_pred = self.forward_pass2Layers(test_embed, weights_input, biases_input,weights_input2, biases_input2, weights_output2)\n",
    "    \n",
    "                l2_loss = tf.math.add_n([\n",
    "                        tf.nn.l2_loss(embed),\n",
    "                        tf.nn.l2_loss(weights_input),\n",
    "                        tf.nn.l2_loss(biases_input),\n",
    "                        tf.nn.l2_loss(weights_input2),\n",
    "                        tf.nn.l2_loss(biases_input2),\n",
    "                        tf.nn.l2_loss(weights_output2),                    \n",
    "                        ],name='adding_l2_losses')\n",
    "            \n",
    "            # two layer forward pass architecture\n",
    "            if layers == 3 :\n",
    "                weights_input2 = tf.Variable(tf.truncated_normal([Config.hidden_size2, Config.hidden_size],    mean=0.0,stddev=0.005),\n",
    "                                            name= 'weights_input2')\n",
    "                biases_input2 = tf.Variable( tf.truncated_normal([Config.hidden_size2,1], mean=0.0,stddev=0.05),\n",
    "                                            name= 'biases_input2')\n",
    "                weights_input3 = tf.Variable(tf.truncated_normal([Config.hidden_size3, Config.hidden_size2],    mean=0.0,stddev=0.005),\n",
    "                                            name= 'weights_input3')\n",
    "                biases_input3 = tf.Variable( tf.truncated_normal([Config.hidden_size3,1], mean=0.0,stddev=0.05),\n",
    "                                            name= 'biases_input3')\n",
    "                weights_output2 = tf.Variable(tf.truncated_normal([parsing_system.numTransitions(),Config.hidden_size3], mean=0.0,stddev=0.005),\n",
    "                                            name= 'weights_output2')\n",
    "                self.prediction = self.forward_pass3Layers(embed, weights_input, biases_input,weights_input2, biases_input2,weights_input3, biases_input3, weights_output2)\n",
    "                self.test_pred = self.forward_pass3Layers(test_embed, weights_input, biases_input,weights_input2, biases_input2,weights_input3, biases_input3, weights_output2)\n",
    "    \n",
    "                l2_loss = tf.math.add_n([\n",
    "                        tf.nn.l2_loss(embed),\n",
    "                        tf.nn.l2_loss(weights_input),\n",
    "                        tf.nn.l2_loss(biases_input),\n",
    "                        tf.nn.l2_loss(weights_input2),\n",
    "                        tf.nn.l2_loss(biases_input2),\n",
    "                        tf.nn.l2_loss(weights_output2),                    \n",
    "                        ],name='adding_l2_losses')\n",
    "            \n",
    "          \n",
    "            # Loss function using L2 Regularization\n",
    "\n",
    "            '''\n",
    "            # mask to clip -1 values\n",
    "            minus1 = tf.constant(-1, dtype=tf.int32)\n",
    "            validIndexes = tf.not_equal(self.train_labels, minus1)\n",
    "            #validIndexes = validIndexes.eval(session=sess)\n",
    "            \n",
    "            self.train_labels = tf.boolean_mask(self.train_labels, validIndexes)\n",
    "            self.prediction = tf.boolean_mask(self.prediction,validIndexes)\n",
    "            self.prediction = tf.Print(self.prediction, [self.prediction], \"self.prediction1\")\n",
    "            self.train_labels = tf.Print(self.train_labels, [self.train_labels], \"self.train_labels1\")\n",
    "            '''\n",
    "\n",
    "\n",
    "            cross_entropy_loss =  tf.reduce_mean(                \n",
    "                    tf.nn.softmax_cross_entropy_with_logits(labels= tf.nn.relu(self.train_labels),\n",
    "                                                            logits=self.prediction,name='calculate_cross_entropy_loss'))\n",
    "#             cross_entropy_loss = tf.Print(cross_entropy_loss, [cross_entropy_loss], \"cross_entropy_loss\")\n",
    "             \n",
    "            \n",
    "\n",
    "            l2_loss = tf.multiply(Config.lam/2.0, l2_loss)            \n",
    "#            l2_loss = tf.Print(l2_loss, [l2_loss], \"l2 loss\")\n",
    "            \n",
    "            self.loss = tf.math.add(cross_entropy_loss, l2_loss)\n",
    "\n",
    "            optimizer = tf.train.GradientDescentOptimizer(Config.learning_rate)\n",
    "            grads = optimizer.compute_gradients(self.loss)\n",
    "            clipped_grads = [(tf.clip_by_norm(grad, 5), var) for grad, var in grads]\n",
    "            self.app = optimizer.apply_gradients(clipped_grads)\n",
    "            \n",
    "            # without gradient clipping \n",
    "#             self.app = optimizer.apply_gradients(grads)\n",
    "\n",
    "            # intializer\n",
    "            self.init = tf.global_variables_initializer()\n",
    "\n",
    "    def train(self, sess, num_steps):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sess:\n",
    "        :param num_steps:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.init.run()\n",
    "        print(\"Initailized\")\n",
    "\n",
    "        average_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            start = (step * Config.batch_size) % len(trainFeats)\n",
    "            end = ((step + 1) * Config.batch_size) % len(trainFeats)\n",
    "            if end < start:\n",
    "                start -= end\n",
    "                end = len(trainFeats)\n",
    "            batch_inputs, batch_labels = trainFeats[start:end], trainLabels[start:end]\n",
    "\n",
    "            feed_dict = {self.train_inputs: batch_inputs, self.train_labels: batch_labels}\n",
    "\n",
    "            _, loss_val = sess.run([self.app, self.loss], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % Config.display_step == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= Config.display_step\n",
    "                print (\"Average loss at step \", step, \": \", average_loss)\n",
    "                average_loss = 0\n",
    "            if step % Config.validation_step == 0 and step != 0:\n",
    "                print (\"\\nTesting on dev set at step \", step)\n",
    "                predTrees = []\n",
    "                for sent in devSents:\n",
    "                    numTrans = parsing_system.numTransitions()\n",
    "\n",
    "                    c = parsing_system.initialConfiguration(sent)\n",
    "                    while not parsing_system.isTerminal(c):\n",
    "                        feat = getFeatures(c)\n",
    "                        pred = sess.run(self.test_pred, feed_dict={self.test_inputs: feat})\n",
    "\n",
    "                        optScore = -float('inf')\n",
    "                        optTrans = \"\"\n",
    "\n",
    "                        for j in range(numTrans):\n",
    "                            if pred[0, j] > optScore and parsing_system.canApply(c, parsing_system.transitions[j]):\n",
    "                                optScore = pred[0, j]\n",
    "                                optTrans = parsing_system.transitions[j]\n",
    "\n",
    "                        c = parsing_system.apply(c, optTrans)\n",
    "\n",
    "                    predTrees.append(c.tree)\n",
    "                result = parsing_system.evaluate(devSents, predTrees, devTrees)\n",
    "                print (result)\n",
    "\n",
    "        print (\"Train Finished.\")\n",
    "\n",
    "    def evaluate(self, sess, testSents):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sess:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        print (\"Starting to predict on test set\")\n",
    "        predTrees = []\n",
    "        for sent in testSents:\n",
    "            numTrans = parsing_system.numTransitions()\n",
    "\n",
    "            c = parsing_system.initialConfiguration(sent)\n",
    "            while not parsing_system.isTerminal(c):\n",
    "                # feat = getFeatureArray(c)\n",
    "                feat = getFeatures(c)\n",
    "                pred = sess.run(self.test_pred, feed_dict={self.test_inputs: feat})\n",
    "\n",
    "                optScore = -float('inf')\n",
    "                optTrans = \"\"\n",
    "\n",
    "                for j in range(numTrans):\n",
    "                    if pred[0, j] > optScore and parsing_system.canApply(c, parsing_system.transitions[j]):\n",
    "                        optScore = pred[0, j]\n",
    "                        optTrans = parsing_system.transitions[j]\n",
    "\n",
    "                c = parsing_system.apply(c, optTrans)\n",
    "\n",
    "            predTrees.append(c.tree)\n",
    "        print( \"Saved the test results.\")\n",
    "        Util.writeConll('result_test.conll', testSents, predTrees)\n",
    "\n",
    "    def forward_pass3Layers(self, embed, weights_input, biases_input,weights_input2, biases_input2, \n",
    "                            weights_input3, biases_input3, weights_output):\n",
    "        \"\"\"\n",
    "\n",
    "        :param embed:\n",
    "        :param weights:\n",
    "        :param biases:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        =======================================================\n",
    "\n",
    "        two layer forward pass function\n",
    "        =======================================================\n",
    "        \"\"\"\n",
    "        print('forwadpass inputs')\n",
    "        print(embed, weights_input, biases_input, weights_input2, biases_input2, weights_output)\n",
    "        \n",
    "        layer1 = tf.add(tf.matmul(weights_input, tf.transpose(embed)),biases_input)\n",
    "        layer1 = tf.math.pow(layer1,3.0)\n",
    "\n",
    "        layer2 = tf.add(tf.matmul(weights_input2, layer1),biases_input2)\n",
    "        layer2 = tf.math.pow(layer2,3.0)\n",
    "        \n",
    "        layer3 = tf.add(tf.matmul(weights_input3, layer2),biases_input2)\n",
    "        layer3 = tf.math.pow(layer3,3.0)\n",
    "\n",
    "        p = tf.matrix_transpose(tf.matmul(weights_output, layer3))\n",
    "         \n",
    "        print('p', p)\n",
    "\n",
    "        return p\n",
    "\n",
    "    def forward_pass2Layers(self, embed, weights_input, biases_input,weights_input2, biases_input2, weights_output):\n",
    "        \"\"\"\n",
    "\n",
    "        :param embed:\n",
    "        :param weights:\n",
    "        :param biases:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        =======================================================\n",
    "\n",
    "        two layer forward pass function\n",
    "        =======================================================\n",
    "        \"\"\"\n",
    "        print('forwadpass inputs')\n",
    "        print(embed, weights_input, biases_input, weights_input2, biases_input2, weights_output)\n",
    "        \n",
    "        layer1 = tf.add(tf.matmul(weights_input, tf.transpose(embed)),biases_input)\n",
    "        layer1 = tf.math.pow(layer1,3.0)\n",
    "\n",
    "        layer2 = tf.add(tf.matmul(weights_input2, layer1),biases_input2)\n",
    "        layer2 = tf.math.pow(layer2,3.0)\n",
    "        \n",
    "\n",
    "        p = tf.matrix_transpose(tf.matmul(weights_output, layer2))\n",
    "         \n",
    "        print('p', p)\n",
    "\n",
    "        return p\n",
    "\n",
    "    def forward_pass(self, embed, weights_input, biases_input, weights_output):\n",
    "        \"\"\"\n",
    "\n",
    "        :param embed:\n",
    "        :param weights:\n",
    "        :param biases:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        =======================================================\n",
    "\n",
    "        Implement the forwrad pass described in\n",
    "        \"A Fast and Accurate Dependency Parser using Neural Networks\"(2014)\n",
    "\n",
    "        =======================================================\n",
    "        \"\"\"\n",
    "        \n",
    "        print('forwadpass inputs')\n",
    "        print(embed, weights_input, biases_input, weights_output)\n",
    "        \n",
    "        layer1 = tf.add(tf.matmul(weights_input, tf.matrix_transpose(embed)),biases_input)\n",
    "\n",
    "# cubic activation function\n",
    "        layer1 = tf.math.pow(layer1,tf.fill(tf.shape(layer1),3.0))\n",
    "\n",
    "# cubic activation function\n",
    "#         layer1 = tf.math.tanh(layer1, name = 'tanh_activation')\n",
    "\n",
    "# cubic activation function\n",
    "#         layer1 = tf.math.sigmoid(layer1, name = 'sigmoid_activation')\n",
    "\n",
    "# relu activation function\n",
    "#         layer1 = tf.nn.relu(layer1, name = 'relu_activation')\n",
    "\n",
    "        \n",
    "        print('layer1',layer1)\n",
    "\n",
    "        p = tf.matrix_transpose(tf.matmul(weights_output, layer1))\n",
    "         \n",
    "        print('p',p)\n",
    "\n",
    "        return p\n",
    "\n",
    "def genDictionaries(sents, trees):\n",
    "    word = []\n",
    "    pos = []\n",
    "    label = []\n",
    "    for s in sents:\n",
    "        for token in s:\n",
    "            word.append(token['word'])\n",
    "            pos.append(token['POS'])\n",
    "\n",
    "    rootLabel = None\n",
    "    for tree in trees:\n",
    "        for k in range(1, tree.n + 1):\n",
    "            if tree.getHead(k) == 0:\n",
    "                rootLabel = tree.getLabel(k)\n",
    "            else:\n",
    "                label.append(tree.getLabel(k))\n",
    "\n",
    "    if rootLabel in label:\n",
    "        label.remove(rootLabel)\n",
    "\n",
    "    index = 0\n",
    "    wordCount = [Config.UNKNOWN, Config.NULL, Config.ROOT]\n",
    "    wordCount.extend(collections.Counter(word))\n",
    "    for word in wordCount:\n",
    "        wordDict[word] = index\n",
    "        index += 1\n",
    "\n",
    "    posCount = [Config.UNKNOWN, Config.NULL, Config.ROOT]\n",
    "    posCount.extend(collections.Counter(pos))\n",
    "    for pos in posCount:\n",
    "        posDict[pos] = index\n",
    "        index += 1\n",
    "\n",
    "    labelCount = [Config.NULL, rootLabel]\n",
    "    labelCount.extend(collections.Counter(label))\n",
    "    for label in labelCount:\n",
    "        labelDict[label] = index\n",
    "        index += 1\n",
    "    print('dict sizes',len(wordDict),len(posDict), len(labelDict))\n",
    "    return wordDict, posDict, labelDict\n",
    "\n",
    "\n",
    "def getWordID(s):\n",
    "    if s in wordDict:\n",
    "        return wordDict[s]\n",
    "    else:\n",
    "        return wordDict[Config.UNKNOWN]\n",
    "\n",
    "\n",
    "def getPosID(s):\n",
    "    if s in posDict:\n",
    "        return posDict[s]\n",
    "    else:\n",
    "        return posDict[Config.UNKNOWN]\n",
    "\n",
    "\n",
    "def getLabelID(s):\n",
    "    if s in labelDict:\n",
    "        return labelDict[s]\n",
    "    else:\n",
    "        return labelDict[Config.UNKNOWN]\n",
    "\n",
    "\n",
    "def getFeatures(c):\n",
    "\n",
    "    \"\"\"\n",
    "    =================================================================\n",
    "\n",
    "    Implement feature extraction described in\n",
    "    \"A Fast and Accurate Dependency Parser using Neural Networks\"(2014)\n",
    "\n",
    "    =================================================================\n",
    "    \"\"\"\n",
    "    s1 = c.getStack(0)\n",
    "    s2 = c.getStack(1)\n",
    "    s3 = c.getStack(2)\n",
    "    \n",
    "    b1 = c.getBuffer(0)\n",
    "    b2 = c.getBuffer(1)\n",
    "    b3 = c.getBuffer(2)\n",
    "\n",
    "    lc1_s1 = c.getLeftChild(s1,1)\n",
    "    rc1_s1 = c.getRightChild(s1,1)\n",
    "    lc2_s1 = c.getLeftChild(s1,2)\n",
    "    rc2_s1 = c.getRightChild(s1,2)\n",
    "    \n",
    "    lc1_s2 = c.getLeftChild(s2,1)\n",
    "    rc1_s2 = c.getRightChild(s2,1)\n",
    "    lc2_s2 = c.getLeftChild(s2,2)\n",
    "    rc2_s2 = c.getRightChild(s2,2)\n",
    "    \n",
    "    lc1_lc1_s1 = c.getLeftChild(lc1_s1, 1)\n",
    "    rc1_rc1_s1 = c.getRightChild(rc1_s1, 1)\n",
    "\n",
    "    lc1_lc1_s2 = c.getLeftChild(lc1_s2, 1)\n",
    "    rc1_rc1_s2 = c.getRightChild(rc1_s2, 1)\n",
    "    \n",
    "    indexs = [s1,s2,s3,b1,b2,b3,lc1_s1,rc1_s1,lc2_s1,rc2_s1,lc1_s2,rc1_s2,lc2_s2,\n",
    "              rc2_s2,lc1_lc1_s1,rc1_rc1_s1,lc1_lc1_s2,rc1_rc1_s2]\n",
    "    \n",
    "    set_w = []    \n",
    "    for i in indexs:\n",
    "        set_w.append(getWordID(c.getWord(i))) \n",
    "    \n",
    "    set_t = []    \n",
    "    for i in indexs:\n",
    "        set_t.append(getPosID(c.getPOS(i))) \n",
    "\n",
    "    set_a = []    \n",
    "    for i in indexs[6:]:\n",
    "        set_a.append(getLabelID(c.getLabel(i))) \n",
    "\n",
    "    features = set_w + set_t + set_a\n",
    "    return features\n",
    "\n",
    "\n",
    "def genTrainExamples(sents, trees):\n",
    "    numTrans = parsing_system.numTransitions()\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    pbar = ProgressBar()\n",
    "    for i in pbar(range(len(sents))):\n",
    "#     for i in pbar(range(10)):\n",
    "        if trees[i].isProjective():\n",
    "            c = parsing_system.initialConfiguration(sents[i])\n",
    "\n",
    "            while not parsing_system.isTerminal(c):\n",
    "                oracle = parsing_system.getOracle(c, trees[i])\n",
    "                feat = getFeatures(c)\n",
    "                label = []\n",
    "                for j in range(numTrans):\n",
    "                    t = parsing_system.transitions[j]\n",
    "                    if t == oracle:\n",
    "                        label.append(1.)\n",
    "                    elif parsing_system.canApply(c, t):\n",
    "                        label.append(0.)\n",
    "                    else:\n",
    "                        label.append(-1.)\n",
    "\n",
    "                if 1.0 not in label:\n",
    "                    print( i, label)\n",
    "                features.append(feat)\n",
    "                labels.append(label)\n",
    "                c = parsing_system.apply(c, oracle)\n",
    "                \n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def load_embeddings(filename, wordDict, posDict, labelDict):\n",
    "    dictionary, word_embeds = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "    embedding_array = np.zeros((len(wordDict) + len(posDict) + len(labelDict), Config.embedding_size))\n",
    "    knownWords = wordDict.keys()\n",
    "    foundEmbed = 0\n",
    "    for i in range(len(embedding_array)):\n",
    "        index = -1\n",
    "        if i < len(knownWords):\n",
    "            w = knownWords[i]\n",
    "            if w in dictionary:\n",
    "                index = dictionary[w]\n",
    "            elif w.lower() in dictionary:\n",
    "                index = dictionary[w.lower()]\n",
    "        if index >= 0:\n",
    "            foundEmbed += 1\n",
    "            embedding_array[i] = word_embeds[index]\n",
    "        else:\n",
    "            embedding_array[i] = np.random.rand(Config.embedding_size) * 0.02 - 0.01\n",
    "    print (\"Found embeddings: \", foundEmbed, \"/\", len(knownWords))\n",
    "\n",
    "    return embedding_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dict sizes', 44392, 48, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% |                                                                        |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Found embeddings: ', 30160, '/', 44392)\n",
      "(44486, 50)\n",
      "root\n",
      "Generating Traning Examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wordDict = {}\n",
    "posDict = {}\n",
    "labelDict = {}\n",
    "parsing_system = None\n",
    "\n",
    "trainSents, trainTrees = Util.loadConll('train.conll')\n",
    "devSents, devTrees = Util.loadConll('dev.conll')\n",
    "testSents, _ = Util.loadConll('test.conll')\n",
    "genDictionaries(trainSents, trainTrees)\n",
    "\n",
    "embedding_filename = '../../word2vec.model'\n",
    "\n",
    "embedding_array = load_embeddings(embedding_filename, wordDict, posDict, labelDict)\n",
    "\n",
    "print(embedding_array.shape)\n",
    "labelInfo = []\n",
    "for idx in np.argsort(labelDict.values()):\n",
    "    labelInfo.append(labelDict.keys()[idx])\n",
    "parsing_system = ParsingSystem(labelInfo[1:])\n",
    "print (parsing_system.rootLabel)\n",
    "\n",
    "print (\"Generating Traning Examples\")\n",
    "trainFeats, trainLabels = genTrainExamples(trainSents, trainTrees)\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "forwadpass inputs\n",
      "(<tf.Tensor 'Reshape:0' shape=(10000, 2400) dtype=float32>, <tf.Variable 'weights_input:0' shape=(200, 2400) dtype=float32_ref>, <tf.Variable 'biases_input:0' shape=(200, 1) dtype=float32_ref>, <tf.Variable 'weights_output:0' shape=(91, 200) dtype=float32_ref>)\n",
      "('layer1', <tf.Tensor 'Pow:0' shape=(200, 10000) dtype=float32>)\n",
      "('p', <tf.Tensor 'matrix_transpose_1/transpose:0' shape=(10000, 91) dtype=float32>)\n",
      "forwadpass inputs\n",
      "(<tf.Tensor 'Reshape_1:0' shape=(1, 2400) dtype=float32>, <tf.Variable 'weights_input:0' shape=(200, 2400) dtype=float32_ref>, <tf.Variable 'biases_input:0' shape=(200, 1) dtype=float32_ref>, <tf.Variable 'weights_output:0' shape=(91, 200) dtype=float32_ref>)\n",
      "('layer1', <tf.Tensor 'Pow_1:0' shape=(200, 1) dtype=float32>)\n",
      "('p', <tf.Tensor 'matrix_transpose_3/transpose:0' shape=(1, 91) dtype=float32>)\n",
      "Initailized\n",
      "('Average loss at step ', 0, ': ', 4.510924816131592)\n",
      "('Average loss at step ', 50, ': ', 4.510911016464234)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(layers)\n",
    "\n",
    "# Build the graph model\n",
    "graph = tf.Graph()\n",
    "model = DependencyParserModel(graph, embedding_array, Config)\n",
    "\n",
    "num_steps = Config.max_iter\n",
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    model.train(sess, num_steps)\n",
    "\n",
    "    model.evaluate(sess, testSents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
